# LLM評価バックエンドの改善タスク

## litellm関連の改善
- [x] 複数プロバイダ（OpenAI, Ollama, Anthropic, Gemini等）への対応強化
- [x] モデル呼び出しのエラーハンドリング改善と再試行機能の実装
- [x] カスタムヘッダーとパラメータのサポート強化
- [x] litellmのキャッシュ機能の活用（同一入力での重複呼び出し防止）
- [ ] litellmの非同期トークンストリーミングに対応（進捗表示の改善）

## 設定関連の改善
- [x] 環境変数による複数のモデルプロバイダ設定のサポート強化
- [x] APIキー管理の安全性向上（.envファイルとシークレット管理）
- [x] デフォルト設定をより柔軟に変更できる仕組みの導入

## 評価機能の拡張
- [ ] メトリクスの拡張（例：ラウンジュ距離、BERTスコア等）
- [ ] ビジネスユースケース向けの評価データセットの追加
- [ ] 評価結果の可視化機能（グラフ出力など）の強化
- [ ] 複数モデルの同時比較機能

## コード品質と開発環境
- [ ] テストの強化と自動テストの導入
- [x] モジュール構造の整理とドキュメンテーションの充実
- [x] パフォーマンス最適化（バッチ処理とタイムアウト処理の改善）
- [x] エラーレポートの詳細化とログ機能の強化
